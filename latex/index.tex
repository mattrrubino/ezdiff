\chapter{EZDiff}
\hypertarget{index}{}\label{index}\index{EZDiff@{EZDiff}}
\label{index_md_README}%
\Hypertarget{index_md_README}%
 A small \href{https://en.wikipedia.org/wiki/Automatic_differentiation}{\texttt{ automatic differentiation}} (AD) library implemented in C++. This library uses C++ operator overloading to automatically construct a static computation graph from mathematical expressions in source code. This graph is evaluated in the forward pass, and the gradient is computed in the backward pass using reverse accumulation AD.\hypertarget{index_autotoc_md1}{}\doxysection{\texorpdfstring{Installation}{Installation}}\label{index_autotoc_md1}
This library works on Unix-\/based operating systems. To install it, execute the following commands\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{git\ clone\ git@github.com:mattrrubino/ezdiff.git}
\DoxyCodeLine{cd\ ezdiff}
\DoxyCodeLine{sudo\ make\ install}

\end{DoxyCode}


To uninstall the library, simply run {\ttfamily make uninstall} from the project directory.\hypertarget{index_autotoc_md2}{}\doxysection{\texorpdfstring{Usage}{Usage}}\label{index_autotoc_md2}
To use this library, you must construct expressions using the {\ttfamily Variable} type. These variables are assigned a value, and you use them to compute derivatives. For example, suppose you wanted to compute the derivative of the following function\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{f(x)\ =\ x\string^2\ +\ 2x\ +\ 1}

\end{DoxyCode}


You could compute the derivative of this function with respect to \$x\$ where \$x = 3\$ using the following code\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <iostream>}}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <ezd/ezd.h>}}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{keywordtype}{int}\ main()\ \{}
\DoxyCodeLine{\ \ \ \ Variable\ x\ =\ ezd::make\_var(3,\ 0);}
\DoxyCodeLine{\ \ \ \ Variable\ f\ =\ x\ *\ x\ +\ 2\ *\ x\ +\ 1;}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ f-\/>forward();}
\DoxyCodeLine{\ \ \ \ f-\/>zero\_grad();}
\DoxyCodeLine{\ \ \ \ f-\/>backward();}
\DoxyCodeLine{}
\DoxyCodeLine{\ \ \ \ \textcolor{comment}{//\ Write\ the\ derivative\ to\ standard\ output}}
\DoxyCodeLine{\ \ \ \ std::cout\ <<\ x-\/>get\_grad()\ <<\ std::endl;}
\DoxyCodeLine{\}}

\end{DoxyCode}


For more examples, see the {\ttfamily test} directory. In particular, the linear regression test shows how this library can be applied to machine learning.\hypertarget{index_autotoc_md3}{}\doxysection{\texorpdfstring{Performance}{Performance}}\label{index_autotoc_md3}
This implementation prioritizes simplicity. It is readable and correct, but it is not performant. Specifically, it suffers from the following issues\+:


\begin{DoxyEnumerate}
\item {\bfseries{Heap Allocations}}\textbackslash{} Each node is stored using a separate heap allocation. Because computational nodes are spread throughout the heap, forward and backward passes suffer from poor cache utilization. To rememdy this, the nodes should be stored in a contiguous region of memory so that the forward and backward passes are cache-\/friendly.
\item {\bfseries{Recursion}}\textbackslash{} The forward and backward passes are implemented recursively. This adds significant function call overhead. To remedy this, the passes should be implemented iteratively.
\item {\bfseries{Polymorphism}}\textbackslash{} The behavior of the forward and backward passes are manipulated polymorphically. This adds virtual function overhead. To remedy this, the passes could be implemented with a switch.
\item {\bfseries{Smart Pointers}}\textbackslash{} Memory is managed using shared pointers. This adds overhead for reference counting. To remedy this, standard pointers could be used with manual freeing.
\item {\bfseries{Sequential}}\textbackslash{} This implementation is sequential. It does not leverage GPUs or SIMD instructions. Parallelizing the code would make it faster for large graphs. 
\end{DoxyEnumerate}